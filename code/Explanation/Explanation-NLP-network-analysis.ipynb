{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YuxuanHuang455/PS-Microexpression/blob/main/Explanation_NLP_network_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7SsKxklpXga"
      },
      "source": [
        "# Part I Query Literature:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUjh_K6cpcik"
      },
      "source": [
        "Retrieve scholarly articles related to \"blockchain\" and \"machine learning\" or \"web3\" and \"AI,\" and store their metadata in a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PxAOId6laDd"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def fetch_arxiv_data(query, max_results=100):\n",
        "    base_url = 'http://export.arxiv.org/api/query?'\n",
        "    search_query = f'search_query=all:{query}&start=0&max_results={max_results}'\n",
        "    response = requests.get(base_url + search_query)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        raise Exception(f\"Failed to fetch data: {response.status_code}\")\n",
        "\n",
        "def parse_arxiv_response(response):\n",
        "    import xml.etree.ElementTree as ET\n",
        "    root = ET.fromstring(response)\n",
        "    ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
        "    entries = []\n",
        "    for entry in root.findall('atom:entry', ns):\n",
        "        title = entry.find('atom:title', ns).text\n",
        "        abstract = entry.find('atom:summary', ns).text\n",
        "        published = entry.find('atom:published', ns).text\n",
        "        year = published.split('-')[0]\n",
        "        journal_ref = entry.find('atom:journal_ref', ns)\n",
        "        venue = journal_ref.text if journal_ref is not None else 'N/A'\n",
        "        entries.append({\n",
        "            'title': title,\n",
        "            'abstract': abstract,\n",
        "            'year': int(year),\n",
        "            'venue': venue\n",
        "        })\n",
        "    return entries\n",
        "\n",
        "queries = ['Microexpression', 'prediction']\n",
        "\n",
        "all_entries = []\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"Fetching data for query: {query}\")\n",
        "    response = fetch_arxiv_data(query)\n",
        "    entries = parse_arxiv_response(response)\n",
        "    all_entries.extend(entries)\n",
        "    time.sleep(3)  # To respect arXiv's rate limits\n",
        "\n",
        "df = pd.DataFrame(all_entries)\n",
        "df.to_csv('literature_data.csv', index=False)\n",
        "print(\"Data saved to literature_data.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgRLReLgz0di"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def fetch_arxiv_data(query, max_results=100):\n",
        "    base_url = 'http://export.arxiv.org/api/query?'\n",
        "    search_query = f'search_query=all:{query}&start=0&max_results={max_results}'\n",
        "    response = requests.get(base_url + search_query)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        raise Exception(f\"Failed to fetch data: {response.status_code}\")\n",
        "\n",
        "def parse_arxiv_response(response):\n",
        "    import xml.etree.ElementTree as ET\n",
        "    root = ET.fromstring(response)\n",
        "    ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
        "    entries = []\n",
        "    for entry in root.findall('atom:entry', ns):\n",
        "        title = entry.find('atom:title', ns).text\n",
        "        abstract = entry.find('atom:summary', ns).text\n",
        "        published = entry.find('atom:published', ns).text\n",
        "        year = published.split('-')[0]\n",
        "        journal_ref = entry.find('atom:journal_ref', ns)\n",
        "        venue = journal_ref.text if journal_ref is not None else 'N/A'\n",
        "        entries.append({\n",
        "            'title': title,\n",
        "            'abstract': abstract,\n",
        "            'year': int(year),\n",
        "            'venue': venue\n",
        "        })\n",
        "    return entries\n",
        "\n",
        "# Interactive input for query\n",
        "query = input(\"Enter your query for arXiv search (e.g., blockchain AND sustainability): \")\n",
        "\n",
        "print(f\"Fetching data for query: {query}\")\n",
        "response = fetch_arxiv_data(query)\n",
        "entries = parse_arxiv_response(response)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df = pd.DataFrame(entries)\n",
        "df.to_csv('literature_data.csv', index=False)\n",
        "print(\"Data saved to literature_data.csv\")\n",
        "print(\"Here are the first few results:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QngUdUaHpfi-"
      },
      "source": [
        "# Part II Natural Language Processing (NLP) Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsIAeFM0pjKa"
      },
      "source": [
        "Perform analyses such as word cloud generation and sentiment analysis on the collected abstracts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbGVnzz7pqyD"
      },
      "source": [
        "## a. Word Cloud Generation\n",
        "Visualize the most frequent words in the abstracts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gKHAnGc1pkoA"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all abstracts into a single string\n",
        "text = ' '.join(df['abstract'].dropna().tolist())\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Abstracts')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4HszvKqpx1l"
      },
      "source": [
        "## b. Sentiment Analysis\n",
        "Analyze the sentiment of each abstract using the TextBlob library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fuoloExp0wq"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Improved sentiment analysis function\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return 'Positive'\n",
        "    elif polarity < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Example DataFrame with abstracts (Replace with your data)\n",
        "data = {'abstract': [\"This is amazing!\", \"I don't like this.\", \"It's okay.\", None]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply sentiment analysis to the abstracts\n",
        "df['sentiment_category'] = df['abstract'].dropna().apply(analyze_sentiment)\n",
        "\n",
        "# Calculate sentiment polarity scores for histogram\n",
        "df['sentiment_polarity'] = df['abstract'].dropna().apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "\n",
        "# Display the results\n",
        "print(df[['abstract', 'sentiment_category', 'sentiment_polarity']])\n",
        "\n",
        "# Plot the sentiment distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "df['sentiment_polarity'].hist(bins=20, edgecolor='black', color='lightblue')\n",
        "plt.title('Sentiment Polarity Distribution of Abstracts')\n",
        "plt.xlabel('Sentiment Polarity')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Plot sentiment category distribution\n",
        "plt.figure(figsize=(7, 5))\n",
        "df['sentiment_category'].value_counts().plot(kind='bar', color=['green', 'gray', 'red'], edgecolor='black')\n",
        "plt.title('Sentiment Category Distribution of Abstracts')\n",
        "plt.xlabel('Sentiment Category')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gcub9C1p41I"
      },
      "source": [
        "## c. Network Visualization\n",
        "Visualize relationships between key terms in the abstracts using NetworkX and matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JecCzquxSRx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def fetch_arxiv_data(query, max_results=100):\n",
        "    base_url = 'http://export.arxiv.org/api/query?'\n",
        "    search_query = f'search_query=all:{query}&start=0&max_results={max_results}'\n",
        "    response = requests.get(base_url + search_query)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        raise Exception(f\"Failed to fetch data: {response.status_code}\")\n",
        "\n",
        "def parse_arxiv_response(response):\n",
        "    import xml.etree.ElementTree as ET\n",
        "    root = ET.fromstring(response)\n",
        "    ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
        "    entries = []\n",
        "    for entry in root.findall('atom:entry', ns):\n",
        "        title = entry.find('atom:title', ns).text\n",
        "        abstract = entry.find('atom:summary', ns).text\n",
        "        published = entry.find('atom:published', ns).text\n",
        "        year = published.split('-')[0]\n",
        "        journal_ref = entry.find('atom:journal_ref', ns)\n",
        "        venue = journal_ref.text if journal_ref is not None else 'N/A'\n",
        "        entries.append({\n",
        "            'title': title,\n",
        "            'abstract': abstract,\n",
        "            'year': int(year),\n",
        "            'venue': venue\n",
        "        })\n",
        "    return entries\n",
        "\n",
        "# Interactive input for query\n",
        "query = input(\"Enter your query for arXiv search (e.g., blockchain AND sustainability): \")\n",
        "\n",
        "print(f\"Fetching data for query: {query}\")\n",
        "response = fetch_arxiv_data(query)\n",
        "entries = parse_arxiv_response(response)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df = pd.DataFrame(entries)\n",
        "df.to_csv('literature_data.csv', index=False)\n",
        "print(\"Data saved to literature_data.csv\")\n",
        "print(\"Here are the first few results:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTcmwCX7Yfn1"
      },
      "outputs": [],
      "source": [
        "print(df['abstract'].head())  # Ensure this column exists and has data\n",
        "print(df['abstract'].isna().sum())  # Check for NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61o0Gepfp7lu"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from networkx.algorithms import community\n",
        "\n",
        "# Tokenize and create a co-occurrence matrix\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(df['abstract'].dropna())\n",
        "Xc = (X.T * X)  # Co-occurrence matrix\n",
        "Xc.setdiag(0)  # Set diagonal to zero\n",
        "\n",
        "# Create graph from co-occurrence matrix\n",
        "G = nx.from_scipy_sparse_array(Xc)\n",
        "\n",
        "# Map indices to words\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "mapping = {i: terms[i] for i in range(len(terms))}\n",
        "G = nx.relabel_nodes(G, mapping)\n",
        "\n",
        "# Filter edges by weight (co-occurrence count)\n",
        "threshold = 100  # Adjust this threshold based on your dataset\n",
        "edges = [(u, v, d) for u, v, d in G.edges(data=True) if d['weight'] > threshold]\n",
        "H = nx.Graph()\n",
        "H.add_edges_from(edges)\n",
        "\n",
        "# Compute node degrees\n",
        "degrees = dict(H.degree())\n",
        "\n",
        "# Detect communities\n",
        "communities = community.greedy_modularity_communities(H)\n",
        "community_map = {}\n",
        "for i, com in enumerate(communities):\n",
        "    for name in com:\n",
        "        community_map[name] = i\n",
        "\n",
        "# Assign colors to communities\n",
        "colors = [community_map[node] for node in H.nodes()]\n",
        "\n",
        "# Draw the network\n",
        "plt.figure(figsize=(15, 15))\n",
        "pos = nx.spring_layout(H, k=0.15, seed=42)  # Seed for reproducibility\n",
        "\n",
        "# Draw nodes with sizes proportional to degree\n",
        "nx.draw_networkx_nodes(H, pos,\n",
        "                       node_size=[degrees[node] * 10 for node in H.nodes()],\n",
        "                       node_color=colors,\n",
        "                       cmap=plt.cm.tab20,\n",
        "                       alpha=0.7)\n",
        "\n",
        "# Draw edges with widths proportional to weight\n",
        "nx.draw_networkx_edges(H, pos,\n",
        "                       width=[d['weight'] * 0.01 for (u, v, d) in H.edges(data=True)],\n",
        "                       alpha=0.5)\n",
        "\n",
        "# Draw node labels\n",
        "nx.draw_networkx_labels(H, pos, font_size=12, font_color='black')\n",
        "\n",
        "# Draw edge labels\n",
        "edge_labels = {(u, v): d['weight'] for u, v, d in H.edges(data=True)}\n",
        "nx.draw_networkx_edge_labels(H, pos, edge_labels=edge_labels, font_size=8)\n",
        "\n",
        "plt.title('Enhanced Co-occurrence Network of Terms in Abstracts')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8NdAikisfUa"
      },
      "source": [
        "Network Visualization:\n",
        "\n",
        "- **Co-occurrence Network Construction**: We employed the CountVectorizer from scikit-learn to tokenize the abstracts and create a term-document matrix. By computing the co-occurrence matrix, we identified pairs of terms that\n",
        "- **Graph Creation and Enhancement:** Using NetworkX, we constructed a graph where nodes represented terms, and edges indicated co-occurrence relationships. To enhance the visualization:\n",
        "\n",
        " - **Node Sizing**: Nodes were sized proportionally to their degree, emphasizing terms with more connections.\n",
        " - **Community Detection**: We applied the greedy modularity algorithm to detect communities within the network, assigning distinct colors to each cluster to highlight related groups of terms.\n",
        " - **Edge Weighting**: Edge widths were adjusted based on co-occurrence frequency, underscoring stronger associations between terms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oum9kVrvyUEq"
      },
      "source": [
        "### Centrality Measures: Identify the most influential terms in the network using centrality measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGAoOqvSyMEL"
      },
      "outputs": [],
      "source": [
        "# Calculate different centrality measures\n",
        "degree_centrality = nx.degree_centrality(H)\n",
        "betweenness_centrality = nx.betweenness_centrality(H)\n",
        "closeness_centrality = nx.closeness_centrality(H)\n",
        "\n",
        "# Highlight top terms based on degree centrality\n",
        "top_terms = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"Top terms by degree centrality:\", [term for term, centrality in top_terms])\n",
        "\n",
        "# Draw the network with node size proportional to degree centrality\n",
        "plt.figure(figsize=(15, 15))\n",
        "pos = nx.spring_layout(H, k=0.15, seed=42)\n",
        "nx.draw_networkx_nodes(H, pos,\n",
        "                       node_size=[degree_centrality[node] * 2000 for node in H.nodes()],\n",
        "                       node_color='skyblue',\n",
        "                       alpha=0.7)\n",
        "nx.draw_networkx_edges(H, pos, alpha=0.5)\n",
        "nx.draw_networkx_labels(H, pos, font_size=10)\n",
        "plt.title('Network with Node Size Proportional to Degree Centrality')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmg1DS_NyWHy"
      },
      "source": [
        "### Clustering Coefficient Analysis: Investigate the local clustering of terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd0TmO44yZGF"
      },
      "outputs": [],
      "source": [
        "# Calculate the clustering coefficient for each node\n",
        "clustering_coefficients = nx.clustering(H)\n",
        "\n",
        "# Highlight nodes with high clustering coefficients\n",
        "high_clustering_nodes = [node for node, coeff in clustering_coefficients.items() if coeff > 0.5]\n",
        "print(\"Nodes with high clustering coefficients:\", high_clustering_nodes)\n",
        "\n",
        "# Draw the network with nodes colored by clustering coefficient\n",
        "plt.figure(figsize=(15, 15))\n",
        "pos = nx.spring_layout(H, k=0.15, seed=42)\n",
        "nx.draw_networkx_nodes(H, pos,\n",
        "                       node_size=100,\n",
        "                       node_color=[clustering_coefficients[node] for node in H.nodes()],\n",
        "                       cmap=plt.cm.viridis,\n",
        "                       alpha=0.7)\n",
        "nx.draw_networkx_edges(H, pos, alpha=0.5)\n",
        "nx.draw_networkx_labels(H, pos, font_size=10)\n",
        "plt.title('Network Colored by Clustering Coefficient')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO1uJrGzyhXn"
      },
      "source": [
        "## Eigenvector Centrality: Highlight nodes based on their influence in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ite5nlEGyfVT"
      },
      "outputs": [],
      "source": [
        "# Calculate eigenvector centrality\n",
        "eigenvector_centrality = nx.eigenvector_centrality(H)\n",
        "\n",
        "# Highlight top terms by eigenvector centrality\n",
        "top_eigenvector_terms = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"Top terms by eigenvector centrality:\", [term for term, centrality in top_eigenvector_terms])\n",
        "\n",
        "# Draw the network with node size proportional to eigenvector centrality\n",
        "plt.figure(figsize=(15, 15))\n",
        "pos = nx.spring_layout(H, k=0.15, seed=42)\n",
        "nx.draw_networkx_nodes(H, pos,\n",
        "                       node_size=[eigenvector_centrality[node] * 2000 for node in H.nodes()],\n",
        "                       node_color='lightgreen',\n",
        "                       alpha=0.7)\n",
        "nx.draw_networkx_edges(H, pos, alpha=0.5)\n",
        "nx.draw_networkx_labels(H, pos, font_size=10)\n",
        "plt.title('Network with Node Size Proportional to Eigenvector Centrality')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHF0yQF6yk-Q"
      },
      "source": [
        "## Core-Periphery Analysis: Identify core and peripheral nodes in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPBlOX5hynjn"
      },
      "outputs": [],
      "source": [
        "# Perform a core-periphery analysis\n",
        "core_number = nx.core_number(H)\n",
        "\n",
        "# Highlight core nodes\n",
        "max_core = max(core_number.values())\n",
        "core_nodes = [node for node, core in core_number.items() if core == max_core]\n",
        "print(\"Core nodes:\", core_nodes)\n",
        "\n",
        "# Draw the network with core nodes highlighted\n",
        "plt.figure(figsize=(15, 15))\n",
        "pos = nx.spring_layout(H, k=0.15, seed=42)\n",
        "nx.draw_networkx_nodes(H, pos,\n",
        "                       nodelist=core_nodes,\n",
        "                       node_size=300,\n",
        "                       node_color='red',\n",
        "                       alpha=0.7,\n",
        "                       label='Core Nodes')\n",
        "nx.draw_networkx_nodes(H, pos,\n",
        "                       nodelist=[node for node in H.nodes() if node not in core_nodes],\n",
        "                       node_size=100,\n",
        "                       node_color='grey',\n",
        "                       alpha=0.7,\n",
        "                       label='Peripheral Nodes')\n",
        "nx.draw_networkx_edges(H, pos, alpha=0.5)\n",
        "nx.draw_networkx_labels(H, pos, font_size=10)\n",
        "plt.title('Core-Periphery Structure in the Network')\n",
        "plt.legend()\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FejYXRWEyq-c"
      },
      "source": [
        "## Hierarchical Community Detection: Use hierarchical clustering to detect communities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtK0XZjoyu6U"
      },
      "outputs": [],
      "source": [
        "# Choose two key nodes based on centrality or domain knowledge\n",
        "node1, node2 = top_terms[0][0], top_terms[1][0]  # Example using top terms by degree centrality\n",
        "\n",
        "# Find the shortest path between the selected nodes\n",
        "shortest_path = nx.shortest_path(H, source=node1, target=node2)\n",
        "print(f\"Shortest path between {node1} and {node2}:\", shortest_path)\n",
        "\n",
        "# Draw the network highlighting the shortest path\n",
        "plt.figure(figsize=(15, 15))\n",
        "pos = nx.spring_layout(H, k=0.15, seed=42)\n",
        "nx.draw_networkx_nodes(H, pos, node_size=100, alpha=0.7)\n",
        "nx.draw_networkx_edges(H, pos, alpha=0.5)\n",
        "nx.draw_networkx_edges(H, pos, edgelist=[(shortest_path[i], shortest_path[i + 1]) for i in range(len(shortest_path) - 1)],\n",
        "                       edge_color='red', width=2, alpha=0.8, label='Shortest Path')\n",
        "nx.draw_networkx_labels(H, pos, font_size=10)\n",
        "plt.title('Network Highlighting the Shortest Path')\n",
        "plt.legend()\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHZc8dflyyG_"
      },
      "source": [
        "## Shortest Path Analysis: Visualize shortest paths between key nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiGiIa2Py1ll"
      },
      "outputs": [],
      "source": [
        "# Choose two key nodes based on centrality or domain knowledge\n",
        "node1, node2 = top_terms[0][0], top_terms[1][0]  # Example using top terms by degree centrality\n",
        "\n",
        "# Find the shortest path between the selected nodes\n",
        "shortest_path = nx.shortest_path(H, source=node1, target=node2)\n",
        "print(f\"Shortest path between {node1} and {node2}:\", shortest_path)\n",
        "\n",
        "# Draw the network highlighting the shortest path\n",
        "plt.figure(figsize=(15, 15))\n",
        "pos = nx.spring_layout(H, k=0.15, seed=42)\n",
        "nx.draw_networkx_nodes(H, pos, node_size=100, alpha=0.7)\n",
        "nx.draw_networkx_edges(H, pos, alpha=0.5)\n",
        "nx.draw_networkx_edges(H, pos, edgelist=[(shortest_path[i], shortest_path[i + 1]) for i in range(len(shortest_path) - 1)],\n",
        "                       edge_color='red', width=2, alpha=0.8, label='Shortest Path')\n",
        "nx.draw_networkx_labels(H, pos, font_size=10)\n",
        "plt.title('Network Highlighting the Shortest Path')\n",
        "plt.legend()\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exy4JWact2I4"
      },
      "source": [
        "# Part III: More on NLP Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUOaQWkVv8b4"
      },
      "outputs": [],
      "source": [
        "!pip install pandas transformers sentence-transformers scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNirClsSt5Pn"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the pre-trained SentenceTransformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for the abstracts\n",
        "embeddings = model.encode(df['abstract'].tolist())\n",
        "\n",
        "# Number of clusters for topic modeling\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "kmeans.fit(embeddings)\n",
        "df['topic'] = kmeans.labels_\n",
        "\n",
        "# Display the topics\n",
        "for i in range(1,num_clusters):\n",
        "    print(f\"Topic {i}:\")\n",
        "    print(df[df['topic'] == i]['abstract'].tolist())\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w25Rw_tmzEsb"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnr2EnJMwEF0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
        "\n",
        "# Only summarize the first 5 abstracts\n",
        "df_test = df.head(5)\n",
        "\n",
        "# Apply summarization with a progress bar\n",
        "summaries = []\n",
        "for abstract in tqdm(df_test['abstract'], desc=\"Summarizing Abstracts (Test)\"):\n",
        "    summary = summarizer(abstract, max_length=50, min_length=25, do_sample=False)[0]['summary_text']\n",
        "    summaries.append(summary)\n",
        "\n",
        "# Add summaries to the test DataFrame\n",
        "df_test['summary'] = summaries\n",
        "\n",
        "# Display original abstracts and their summaries\n",
        "for idx, row in df_test.iterrows():\n",
        "    print(f\"Original Abstract: {row['abstract']}\")\n",
        "    print(f\"Summary: {row['summary']}\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
